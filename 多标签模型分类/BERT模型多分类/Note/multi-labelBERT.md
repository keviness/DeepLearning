> ğŸ”— åŸæ–‡é“¾æ¥ï¼š [https://cloud.tencent.com/developer...](https://cloud.tencent.com/developer/article/1814597)

## ä»‹ç»

[è‡ªç„¶è¯­è¨€å¤„ç† ](https://cloud.tencent.com/product/nlp?from=10680)(NLP)æ˜¯ä¸€ç§å°†éç»“æ„åŒ–æ–‡æœ¬å¤„ç†æˆæœ‰æ„ä¹‰çš„çŸ¥è¯†çš„äººå·¥æ™ºèƒ½æŠ€æœ¯ã€‚NLPè§£å†³äº†åˆ†ç±»ã€ä¸»é¢˜å»ºæ¨¡ã€æ–‡æœ¬ç”Ÿæˆã€é—®ç­”ã€æ¨èç­‰ä¸šåŠ¡é—®é¢˜ã€‚è™½ç„¶TF/IDFçŸ¢é‡åŒ–æˆ–å…¶ä»–é«˜çº§è¯åµŒå…¥(å¦‚GLOVEå’ŒWord2Vec)åœ¨æ­¤ç±»NLPä¸šåŠ¡é—®é¢˜ä¸Šè¡¨ç°å‡ºäº†è‰¯å¥½çš„æ€§èƒ½ï¼Œä½†è¿™äº›æ¨¡å‹å­˜åœ¨å±€é™æ€§å°±æ˜¯ä½¿ç”¨ä¸€ä¸ªå‘é‡å¯¹è¯è¿›è¡Œç¼–ç è€Œä¸è€ƒè™‘ä¸Šä¸‹æ–‡çš„ä¸åŒå«ä¹‰ã€‚å› æ­¤ï¼Œå½“è¯•å›¾è§£å†³ç†è§£ç”¨æˆ·æ„å›¾æ‰€éœ€çš„é—®é¢˜æ—¶ï¼Œè¿™äº›æ¨¡å‹å¯èƒ½ä¸èƒ½å¾ˆå¥½åœ°æ‰§è¡Œã€‚ä¸€ä¸ªä¾‹å­æ˜¯ï¼Œå½“ç”¨æˆ·ä¸è‡ªåŠ¨èŠå¤©æœºå™¨äººäº¤äº’æ—¶ï¼Œå®ƒè¯•å›¾ç†è§£ç”¨æˆ·æŸ¥è¯¢çš„æ„å›¾å¹¶å‡†ç¡®åœ°æä¾›å“åº”ã€‚

å¯¹äºè¿™ç§æƒ…å†µï¼ŒNLPä¸­çš„å¦ä¸€ä¸ªä¾‹å­æ˜¯ä»ä¸‹é¢ä¸¤ä¸ªå¥å­ä¸­è§£ç ä¸Šä¸‹æ–‡æ„ä¹‰ã€‚

1. A thieve robbed a bank.
2. He went to river bank.

ä»ä»¥ä¸Šä¸¤ç§è¡¨è¿°ä¸­ï¼Œäººä»¬å¾ˆå®¹æ˜“å°±èƒ½çœ‹å‡ºâ€œbankâ€æœ‰ä¸¤ç§ä¸åŒçš„å«ä¹‰;ç„¶è€Œï¼Œæœºå™¨ä¸èƒ½åŒºåˆ†ï¼Œå› ä¸ºä¸Šé¢æåˆ°çš„è¯åµŒå…¥ä½¿ç”¨ç›¸åŒçš„æ ‡è®°â€œbankâ€ï¼Œè€Œä¸ç®¡ä»–ä»¬çš„ä¸Šä¸‹æ–‡æ„ä¹‰ã€‚ä¸ºäº†å…‹æœè¿™ä¸€æŒ‘æˆ˜ï¼Œè°·æ­Œä»Transformers (BERT)æ¨¡å‹å¼€å‘äº†æœ€å…ˆè¿›çš„åŒå‘ç¼–ç å™¨è¡¨ç¤ºã€‚

## BERTæ˜¯ä»€ä¹ˆ?

BERTæ˜¯åœ¨8äº¿å•è¯çš„å›¾ä¹¦è¯­æ–™åº“å’Œ2500ä¸‡å•è¯çš„è‹±è¯­ç»´åŸºç™¾ç§‘ä¸Šè®­ç»ƒçš„é¢„è®­ç»ƒæ¨¡å‹ã€‚åœ¨BERTä¸­ï¼Œâ€œbankâ€å°†æœ‰ä¸¤ä¸ªä¸åŒçš„å«ä¹‰ï¼Œå› ä¸ºå®ƒä»¬çš„ä¸Šä¸‹æ–‡å·®å¼‚ã€‚åœ¨ä¿æŒNLPä»»åŠ¡çš„é«˜æ€§èƒ½çš„åŒæ—¶å¹¶ä¸ä¼šé™ä½æ¨¡å‹æ„å»ºçš„è®­ç»ƒæ—¶é—´ã€‚å¹¶ä¸”å¯ä»¥ä»BERTä¸­æå–æ–°çš„è¯­è¨€ç‰¹å¾ç”¨äºæ¨¡å‹é¢„æµ‹ã€‚ä¸RNNã€LSTMã€CNNç­‰æ·±åº¦å­¦ä¹ æ¨¡å‹ç›¸æ¯”ï¼ŒBERTçš„å‘å±•é€Ÿåº¦è¦å¿«å¾—å¤šã€‚ä½œä¸ºé«˜å±‚æ¬¡çš„ç†è§£ï¼ŒBERTæœ‰ä¸¤ç§ä¸åŒçš„æ¶æ„å˜ä½“:BERT baseå’ŒBERT largeã€‚ç¬¬ä¸€ä¸ªå˜å‹æœ‰12ä¸ªTransformers å—ï¼Œ12ä¸ªæ³¨æ„å¤´ï¼Œ1.1äº¿å‚æ•°ï¼Œåä¸€ä¸ªå˜å‹æœ‰24ä¸ªTransformers ï¼Œ16ä¸ªæ³¨æ„å¤´ï¼Œ3.4äº¿å‚æ•°ã€‚å®ƒåœ¨ä½¿ç”¨è¿‡ç¨‹ä¸­å®Œæˆäº†ä¸¤ä¸ªNLPçš„ä»»åŠ¡:é®è”½è¯­è¨€å»ºæ¨¡å’Œä¸‹ä¸€å¥é¢„æµ‹ã€‚

## æ•°æ®é›†

ä»æ­¤å¤„ï¼ˆhttps://datahack.analyticsvidhya.com/contest/janatahack-independence-day-2020-ml-hackathon/#ProblemStatementï¼‰è·å–æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†å¯ç”¨äºç ”ç©¶è®ºæ–‡çš„ä¸»é¢˜å»ºæ¨¡çš„å¤šæ ‡ç­¾åˆ†ç±»å¯¹æ¯”ã€‚å¯¹æ¯”çš„ç›®çš„æ˜¯ä»å¤§å‹çš„ç§‘å­¦æ–‡ç« åœ¨çº¿å­˜æ¡£ä¸­å°½å¯èƒ½åœ°å®¹æ˜“æ‰¾åˆ°ç›¸å…³çš„æ–‡ç« ã€‚æˆ‘é€‰æ‹©æ­¤æ•°æ®é›†çš„åŸå› æ˜¯ï¼Œå°½ç®¡æœ‰è®¸å¤šå…³äºäºŒè¿›åˆ¶åˆ†ç±»çš„Twitteræƒ…ç»ªè®¨è®ºBERTå’ŒPytorchçš„æ–‡ç« ï¼Œä½†å¾ˆå°‘æ‰¾åˆ°æœ‰å…³å¤„ç†å¤šç±»é—®é¢˜çš„ã€‚å¹¶ä¸”æœ‰å¾ˆå¤šå…±äº«ä»£ç å¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œã€‚

æŸ¥çœ‹å¦‚ä¸‹çš„ä»£ç æˆ‘å»ºè®®å…·å¤‡pythonï¼ŒNLPï¼Œæ·±åº¦å­¦ä¹ å’ŒPytorchæ¡†æ¶çš„åŸºç¡€çŸ¥è¯†ã€‚å¿…é¡»ä½¿ç”¨Googleå¸æˆ·æ‰èƒ½ä½¿ç”¨Google Colabå¸æˆ·ã€‚

## å¤„ç†æ•°æ®çš„æ–¹æ³•

åœ¨ä¼ ç»Ÿçš„NLPæœºå™¨å­¦ä¹ é—®é¢˜ä¸­ï¼Œæˆ‘ä»¬å€¾å‘äºæ¸…é™¤ä¸éœ€è¦çš„æ–‡æœ¬ï¼Œä¾‹å¦‚åˆ é™¤åœç”¨è¯ï¼Œæ ‡ç‚¹ç¬¦å·ï¼Œåˆ é™¤ç¬¦å·å’Œæ•°å­—ç­‰ã€‚ä½†æ˜¯ï¼Œåœ¨BERTä¸­ï¼Œä¸éœ€è¦æ‰§è¡Œæ­¤ç±»é¢„å¤„ç†ä»»åŠ¡ï¼Œå› ä¸ºBERTä½¿ç”¨äº†è¿™äº› å•è¯çš„é¡ºåºå’Œä½ç½®ï¼Œä»¥äº†è§£ç”¨æˆ·è¾“å…¥çš„æ„å›¾ã€‚

ML / DLå·¥ç¨‹å¸ˆåº”è¯¥ä»ä¸åŒæ–¹é¢æ¢ç´¢æ•°æ®é›†ï¼Œä»¥çœŸæ­£äº†è§£ä»–ä»¬æ‰‹ä¸­çš„æ•°æ®ç±»å‹ï¼Œè¿™æ˜¯ä¸€ä¸ªå¥½ä¹ æƒ¯ã€‚NLPçš„å…¸å‹åŠŸèƒ½æ˜¯å•è¯è®¡æ•°ï¼ŒåŠ¨è¯è®¡æ•°ï¼Œå½¢å®¹è¯è®¡æ•°ï¼Œæ•°å­—è®¡æ•°ï¼Œæ ‡ç‚¹ç¬¦å·è®¡æ•°ï¼ŒåŒå­—æ¯ç»„è®¡æ•°ï¼Œä¸‰å­—ç»„è®¡æ•°ç­‰ã€‚ä¸ºç®€ä¾¿èµ·è§ï¼Œæˆ‘å·²å±•ç¤ºäº†å¦‚ä½•å¯¹å•è¯è®¡æ•°åˆ—è¿›è¡Œè®¡æ•°ï¼Œå…¶ä¸­å•ä¸ªæ ‡é¢˜ä¸­ä½¿ç”¨çš„æ€»å•è¯æ•°å°†è¢«è®¡ç®—åœ¨å†…ã€‚æ‚¨å¯èƒ½è¿˜éœ€è¦å¤„ç†ç±»ä¼¼äºTITLEçš„Abstractåˆ—ï¼Œä»¥åŠABSTRACTå’ŒTITLEçš„ç»„åˆã€‚

ä¸‹é¢çš„å‘½ä»¤åˆ›å»ºâ€œ WORD_COUNTâ€åˆ—ã€‚

```JavaScript
 df_raw['WORD_COUNT'] = df_raw['TITLE'].apply(lambda x: len(x.split())
```

è¿™å°†ç”Ÿæˆâ€œ WORD_COUNTâ€çš„åˆ†å¸ƒå›¾ï¼Œå³æ ‡é¢˜çš„é•¿åº¦ã€‚

å¦‚æ‚¨æ‰€è§ï¼Œæ–‡ç« æ ‡é¢˜çš„å¤§éƒ¨åˆ†ä»¥10ä¸ªå•è¯ä¸ºä¸­å¿ƒï¼Œè¿™æ˜¯é¢„æœŸçš„ç»“æœï¼Œå› ä¸ºTITLEåº”è¯¥ç®€çŸ­ï¼Œç®€æ´ä¸”æœ‰æ„ä¹‰ã€‚

ç”±äºæˆ‘å°†ä»…ä½¿ç”¨â€œ TITLEâ€å’Œâ€œ target_listâ€ï¼Œå› æ­¤æˆ‘åˆ›å»ºäº†ä¸€ä¸ªåä¸ºdf2çš„æ–°æ•°æ®æ¡†ã€‚df2.headï¼ˆï¼‰å‘½ä»¤æ˜¾ç¤ºè®­ç»ƒæ•°æ®é›†ä¸­çš„å‰äº”ä¸ªè®°å½•ã€‚å¦‚æ‚¨æ‰€è§ï¼Œä¸¤ä¸ªç›®æ ‡æ ‡ç­¾è¢«æ ‡è®°åˆ°æœ€åçš„è®°å½•ï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆè¿™ç§é—®é¢˜ç§°ä¸ºå¤šæ ‡ç­¾åˆ†ç±»é—®é¢˜çš„åŸå› ã€‚

```JavaScript
 df2 = df_raw[['TITLE', 'target_list']].copy()
 
 df2.head()
```

![](https://fjjwhjwd3p.feishu.cn/space/api/box/stream/download/asynccode/?code=ZGRmZWZjOTVjMzY4MmJlZWNhMzU4OTg1YTEyMDRhMTdfbFdpWHNlcEdSV1I4VnlrMFhSaDJZQTQweGl2Qzh3Y1VfVG9rZW46Ym94Y25pazVMclNQQmhGQ1lhenpLb29hcDhlXzE2Njg4NjU3OTU6MTY2ODg2OTM5NV9WNA)

åŒæ—¶ï¼Œè®¾ç½®å°†ç”¨äºæ¨¡å‹è®­ç»ƒçš„å‚æ•°ã€‚ç”±äºæˆ‘æ›´å–œæ¬¢ä½¿ç”¨2*baseæ•°å­—ï¼Œå› æ­¤æœ€å¤§é•¿åº¦è®¾ç½®ä¸º16ï¼Œè¿™æ¶µç›–äº†å¤§éƒ¨åˆ†â€œ TITLEâ€é•¿åº¦ã€‚è®­ç»ƒå’Œæœ‰æ•ˆæ‰¹å¤„ç†å¤§å°è®¾ç½®ä¸º32ã€‚epochä¸º4ï¼Œå› ä¸ºå®ƒå¾ˆå®¹æ˜“åœ¨å‡ ä¸ªepochä¸Šè¿‡åº¦æ‹Ÿåˆã€‚æˆ‘ä»lr=0.00001å¼€å§‹å­¦ä¹ ã€‚æ‚¨å¯ä»¥éšæ„å°è¯•ä¸åŒçš„å€¼ä»¥æé«˜å‡†ç¡®æ€§ã€‚

```JavaScript
 # Sections of config
 # Defining some key variables that will be used later on in the training
 MAX_LEN = 16
 TRAIN_BATCH_SIZE = 32
 VALID_BATCH_SIZE = 32
 EPOCHS = 4
 LEARNING_RATE = 1e-05
 tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
```

è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªç§°ä¸ºâ€œ CustomDatasetâ€çš„é€šç”¨ç±»ã€‚Classä»æˆ‘ä»¬çš„åŸå§‹è¾“å…¥ç‰¹å¾ç”Ÿæˆå¼ é‡ï¼Œå¹¶ä¸”Pytorchå¼ é‡å¯ä»¥æ¥å—classçš„è¾“å‡ºã€‚å®ƒæœŸæœ›å…·æœ‰ä¸Šé¢å®šä¹‰çš„â€œ TITLEâ€ï¼Œâ€œ target_listâ€ï¼Œmax_lenï¼Œå¹¶ä½¿ç”¨BERT toknizer.encode_pluså‡½æ•°å°†è¾“å…¥è®¾ç½®ä¸ºæ•°å­—çŸ¢é‡æ ¼å¼ï¼Œç„¶åè½¬æ¢ä¸ºå¼ é‡æ ¼å¼è¿”å›ã€‚

```JavaScript
 class CustomDataset(Dataset):
     def __init__(self, dataframe, tokenizer, max_len):
         self.tokenizer = tokenizer
         self.data = dataframe
         self.title = dataframe['TITLE']
         self.targets = self.data.target_list
         self.max_len = max_len
 
     def __len__(self):
         return len(self.title)
 
 
 
     def __getitem__(self, index):
         title = str(self.title[index])
         title = " ".join(title.split())
 
         inputs = self.tokenizer.encode_plus(
             title,
             None,
             add_special_tokens=True,
             max_length=self.max_len,
             padding='max_length',
             return_token_type_ids=True,
             truncation=True
 
         )
 
         ids = inputs['input_ids']
         mask = inputs['attention_mask']
         token_type_ids = inputs["token_type_ids"]
         return {
             'ids': torch.tensor(ids, dtype=torch.long),
             'mask': torch.tensor(mask, dtype=torch.long),
             'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),
             'targets': torch.tensor(self.targets[index], dtype=torch.float)
 
         }
```

æ•°æ®é›†çš„80ï¼…ç”¨äºæ¨¡å‹è®­ç»ƒï¼Œè€Œ20ï¼…ç”¨äºéªŒè¯ã€‚æµ‹è¯•æ•°æ®é›†å®Œå…¨ç”¨äºæµ‹è¯•ç›®çš„ã€‚

```JavaScript
 train_size = 0.8
 train_dataset = df2.sample(frac=train_size,random_state=200)
 valid_dataset = df2.drop(train_dataset.index).reset_index(drop=True)
 train_dataset = train_dataset.reset_index(drop=True)
 
 
 print("FULL Dataset: {}".format(df2.shape))
 print("TRAIN Dataset: {}".format(train_dataset.shape))
 print("TEST Dataset: {}".format(valid_dataset.shape))
 
 
 training_set = CustomDataset(train_dataset, tokenizer, MAX_LEN)
 validation_set = CustomDataset(valid_dataset, tokenizer, MAX_LEN)
```

![](https://fjjwhjwd3p.feishu.cn/space/api/box/stream/download/asynccode/?code=MDQ4YTc1NDNjYmI4MjlhYmRmYTE0NGNlY2IxYTcyNzFfSUczUU1GaTFWYzFveWtadEVWZGdHcmlLV0dVRkd4d21fVG9rZW46Ym94Y25YWGxnblZja2xkRkpGTklKQ1Z2bWZlXzE2Njg4NjU3OTU6MTY2ODg2OTM5NV9WNA)

æˆ‘ä»¬å·²ç»è®¨è®ºäº†å°†å¼ é‡å‡†å¤‡ä¸ºè¾“å…¥ç‰¹å¾çš„å¤§éƒ¨åˆ†åŸºç¡€å·¥ä½œã€‚ç°åœ¨ï¼Œæ„å»ºBERTæ¨¡å‹å¾ˆå®¹æ˜“ã€‚ç”±äºæ¥è‡ªæ¨¡å‹çš„å†—é•¿è¾“å‡ºï¼Œæˆ‘å·²ç®€åŒ–ä¸ºä»…æ˜¾ç¤ºæ¨¡å‹ã€‚æˆ‘å·²ä½¿ç”¨dropout 0.3æ¥éšæœºå‡å°‘ç‰¹å¾ï¼Œä»¥æœ€å¤§ç¨‹åº¦åœ°å‡å°‘ç¬¬2å±‚çš„è¿‡æ‹Ÿåˆã€‚ç¬¬3å±‚é‡‡ç”¨äº†768ç»´ç‰¹å¾ï¼Œè¿™äº›ç‰¹å¾æ˜¯ä»ä½¿ç”¨BERTçš„ç¬¬2å±‚è¾“å‡ºçš„ã€‚å®ƒè¿”å›6ä¸ªç‰¹å¾ï¼Œè¿™æ˜¯å¯¹ç›®æ ‡åˆ—è¡¨çš„æœ€ç»ˆé¢„æµ‹ã€‚

```JavaScript
 class BERTClass(torch.nn.Module):
     def __init__(self):
         super(BERTClass, self).__init__()
         self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased')
         self.l2 = torch.nn.Dropout(0.3)
         self.l3 = torch.nn.Linear(768, 6)
 
     def forward(self, ids, mask, token_type_ids):
         _, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids)
         output_2 = self.l2(output_1)
         output = self.l3(output_2)
         return output
 
 model = BERTClass()
 model.to(device)
```

BCEæŸå¤±å‡½æ•°ç”¨äºæ‰¾å‡ºæ¨¡å‹é¢„æµ‹å€¼ä¸å®é™…ç›®æ ‡å€¼ä¹‹é—´çš„è¯¯å·®ã€‚ä½¿ç”¨Adamä¼˜åŒ–å™¨ã€‚æŸå¤±åŠŸèƒ½è¯·å‚è§ä¸‹æ–‡ã€‚

```JavaScript
 def loss_fn(outputs, targets):
     return torch.nn.BCEWithLogitsLoss()(outputs, targets)
 
 optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)
```

åŒæ—¶æˆ‘è¿˜åˆ›å»ºäº†æ£€æŸ¥ç‚¹ï¼Œå¯ä»¥åœ¨è®­ç»ƒæœŸé—´ä¿å­˜æœ€ä½³æ¨¡å‹ã€‚å½“éœ€è¦ä»åœä¸‹æ¥çš„åœ°æ–¹ç»§ç»­è®­ç»ƒæ—¶ï¼Œè¿™å°†æœ‰åŠ©äºå‡å°‘è®­ç»ƒæ—¶é—´ã€‚åˆ›å»ºæ£€æŸ¥ç‚¹å¯ä»¥èŠ‚çœæ—¶é—´ï¼Œä»¥ä¾¿ä»å¤´å¼€å§‹è¿›è¡Œé‡æ–°è®­ç»ƒã€‚å¦‚æœæ‚¨å¯¹ä»æœ€ä½³æ¨¡å‹ç”Ÿæˆçš„è¾“å‡ºæ„Ÿåˆ°æ»¡æ„ï¼Œåˆ™ä¸éœ€è¦è¿›ä¸€æ­¥çš„å¾®è°ƒï¼Œåˆ™å¯ä»¥ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ¨æ–­ã€‚

```JavaScript
 def load_ckp(checkpoint_fpath, model, optimizer):
 
     """
     checkpoint_path: path to save checkpoint
     model: model that we want to load checkpoint parameters into     
     optimizer: optimizer we defined in previous training
 
     """
     # load check point
     checkpoint = torch.load(checkpoint_fpath)
     # initialize state_dict from checkpoint to model
     model.load_state_dict(checkpoint['state_dict'])
     # initialize optimizer from checkpoint to optimizer
     optimizer.load_state_dict(checkpoint['optimizer'])
     # initialize valid_loss_min from checkpoint to valid_loss_min
     valid_loss_min = checkpoint['valid_loss_min']
     # return model, optimizer, epoch value, min validation loss 
     return model, optimizer, checkpoint['epoch'], valid_loss_min.item()
 import shutil, sys   
 
 def save_ckp(state, is_best, checkpoint_path, best_model_path):
 
     """
     state: checkpoint we want to save
     is_best: is this the best checkpoint; min validation loss
     checkpoint_path: path to save checkpoint
     best_model_path: path to save best model
     """
     f_path = checkpoint_path
     # save checkpoint data to the path given, checkpoint_path
     torch.save(state, f_path)
     # if it is a best model, min validation loss
     if is_best:
         best_fpath = best_model_path
         # copy that checkpoint file to best path given, best_model_path
         shutil.copyfile(f_path, best_fpath)
 def train_model(start_epochs,  n_epochs, valid_loss_min_input, 
                 training_loader, validation_loader, model, 
                 optimizer, checkpoint_path, best_model_path):
 
   # initialize tracker for minimum validation loss
   valid_loss_min = valid_loss_min_input 
   for epoch in range(start_epochs, n_epochs+1):
     train_loss = 0
     valid_loss = 0
     model.train()
     print('############# Epoch {}: Training Start   #############'.format(epoch))
     for batch_idx, data in enumerate(training_loader):
         #print('yyy epoch', batch_idx)
         ids = data['ids'].to(device, dtype = torch.long)
         mask = data['mask'].to(device, dtype = torch.long)
         token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)
         targets = data['targets'].to(device, dtype = torch.float)
         outputs = model(ids, mask, token_type_ids)
         optimizer.zero_grad()
         loss = loss_fn(outputs, targets)
         #if batch_idx%5000==0:
          #   print(f'Epoch: {epoch}, Training Loss:  {loss.item()}')
         optimizer.zero_grad()
         loss.backward()
         optimizer.step()
         #print('before loss data in training', loss.item(), train_loss)
         train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.item() - train_loss))
         #print('after loss data in training', loss.item(), train_loss)
     print('############# Epoch {}: Training End     #############'.format(epoch))
     print('############# Epoch {}: Validation Start   #############'.format(epoch))
     ######################  
     # validate the model #
     ######################
     model.eval()
     with torch.no_grad():
       for batch_idx, data in enumerate(validation_loader, 0):
             ids = data['ids'].to(device, dtype = torch.long)
             mask = data['mask'].to(device, dtype = torch.long)
             token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)
             targets = data['targets'].to(device, dtype = torch.float)
             outputs = model(ids, mask, token_type_ids)
             loss = loss_fn(outputs, targets)
             valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.item() - valid_loss))
             val_targets.extend(targets.cpu().detach().numpy().tolist())
             val_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())
 
       print('############# Epoch {}: Validation End     #############'.format(epoch))
       # calculate average losses
       #print('before cal avg train loss', train_loss)
       train_loss = train_loss/len(training_loader)
       valid_loss = valid_loss/len(validation_loader)
       # print training/validation statistics 
       print('Epoch: {} \tAvgerage Training Loss: {:.6f} \tAverage Validation Loss: {:.6f}'.format(
             epoch, 
             train_loss,
             valid_loss
             ))
       # create checkpoint variable and add important data
       checkpoint = {
             'epoch': epoch + 1,
             'valid_loss_min': valid_loss,
             'state_dict': model.state_dict(),
             'optimizer': optimizer.state_dict()
       }
 
         # save checkpoint
       save_ckp(checkpoint, False, checkpoint_path, best_model_path)
       ## TODO: save the model if validation loss has decreased
       if valid_loss <= valid_loss_min:
         print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,valid_loss))
         # save checkpoint as best model
         save_ckp(checkpoint, True, checkpoint_path, best_model_path)
         valid_loss_min = valid_loss
     print('############# Epoch {}  Done   #############\n'.format(epoch))
   return model
```

â€œtrain_modelâ€è¢«åˆ›å»ºæ¥è®­ç»ƒæ¨¡å‹ï¼Œâ€œcheckpoint_pathâ€æ˜¯è®­ç»ƒæ¨¡å‹çš„å‚æ•°å°†è¢«ä¿å­˜ä¸ºæ¯ä¸ªepochï¼Œâ€œbest_modelâ€æ˜¯æœ€å¥½çš„æ¨¡å‹å°†è¢«ä¿å­˜çš„åœ°æ–¹ã€‚

```JavaScript
 checkpoint_path = '/content/drive/My Drive/NLP/ResearchArticlesClassification/checkpoint/current_checkpoint.pt'
 
 best_model = '/content/drive/My Drive/NLP/ResearchArticlesClassification/best_model/best_model.pt'
 
 trained_model = train_model(1, 4, np.Inf, training_loader, validation_loader, model, 
                       optimizer,checkpoint_path,best_model)
```

è®­ç»ƒç»“æœå¦‚ä¸‹ï¼š

```JavaScript
 ############# Epoch 1: Training Start   #############
 ############# Epoch 1: Training End     #############
 ############# Epoch 1: Validation Start   #############
 ############# Epoch 1: Validation End     #############
 Epoch: 1    Avgerage Training Loss: 0.000347    Average Validation Loss: 0.001765
 Validation loss decreased (inf --> 0.001765).  Saving model ...
 ############# Epoch 1  Done   #############
 
 ############# Epoch 2: Training Start   #############
 ############# Epoch 2: Training End     #############
 ############# Epoch 2: Validation Start   #############
 ############# Epoch 2: Validation End     #############
 Epoch: 2    Avgerage Training Loss: 0.000301    Average Validation Loss: 0.001831
 ############# Epoch 2  Done   #############
 
 ############# Epoch 3: Training Start   #############
 ############# Epoch 3: Training End     #############
 ############# Epoch 3: Validation Start   #############
 ############# Epoch 3: Validation End     #############
 Epoch: 3    Avgerage Training Loss: 0.000263    Average Validation Loss: 0.001896
 ############# Epoch 3  Done   #############
 
 ############# Epoch 4: Training Start   #############
 ############# Epoch 4: Training End     #############
 ############# Epoch 4: Validation Start   #############
 ############# Epoch 4: Validation End     #############
 Epoch: 4    Avgerage Training Loss: 0.000228    Average Validation Loss: 0.002048
 ############# Epoch 4  Done   #############
```

å› ä¸ºæˆ‘åªæ‰§è¡Œäº†4ä¸ªepochï¼Œæ‰€ä»¥å®Œæˆå¾—å¾ˆå¿«ï¼Œæˆ‘å°†thresholdè®¾ç½®ä¸º0.5ã€‚ä½ å¯ä»¥è¯•è¯•è¿™ä¸ªé˜ˆå€¼ï¼Œçœ‹çœ‹æ˜¯å¦èƒ½æé«˜ç»“æœã€‚

```JavaScript
 val_preds = (np.array(val_outputs) > 0.5).astype(int)
 val_preds
 array([[0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0],
        ...,
        [0, 0, 1, 0, 0, 0],
        [0, 1, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0]])
```

è®©æˆ‘ä»¬å°†ç²¾ç¡®åº¦å’ŒF1å¾—åˆ†å®šä¹‰ä¸ºæ¨¡å‹æ€§èƒ½çš„æŒ‡æ ‡ã€‚F1å°†è¢«ç”¨äºè¯„ä¼°ã€‚

```JavaScript
 accuracy = metrics.accuracy_score(val_targets, val_preds)
 f1_score_micro = metrics.f1_score(val_targets, val_preds, average='micro')
 f1_score_macro = metrics.f1_score(val_targets, val_preds, average='macro')
 print(f"Accuracy Score = {accuracy}")
 print(f"F1 Score (Micro) = {f1_score_micro}")
 print(f"F1 Score (Macro) = {f1_score_macro}")
```

![](https://fjjwhjwd3p.feishu.cn/space/api/box/stream/download/asynccode/?code=NjQ4NzM0Yjc1YjA3ZWI1N2U1ODk3MTBlNzExNTdkYWZfa2kyS3hKR21OWTFLU2Y1T2dSdm1WTHdaSTYybFZ4Q29fVG9rZW46Ym94Y252ZGF1c1NyUDhBOHJSOUZxNVQ5ZVlnXzE2Njg4NjU3OTU6MTY2ODg2OTM5NV9WNA)

ä½¿ç”¨æ··æ·†çŸ©é˜µå’Œåˆ†ç±»æŠ¥å‘Šï¼Œä»¥å¯è§†åŒ–æˆ‘ä»¬çš„æ¨¡å‹å¦‚ä½•æ­£ç¡®/ä¸æ­£ç¡®åœ°é¢„æµ‹æ¯ä¸ªå•ç‹¬çš„ç›®æ ‡ã€‚

```JavaScript
 from sklearn.metrics import multilabel_confusion_matrix as mcm, classification_report
 cm_labels = ['Computer Science', 'Physics', 'Mathematics',
        'Statistics', 'Quantitative Biology', 'Quantitative Finance']
 cm = mcm(val_targets, val_preds)
 print(classification_report(val_targets, val_preds))
```

![](https://fjjwhjwd3p.feishu.cn/space/api/box/stream/download/asynccode/?code=YTk0NGI1Zjg1NTA4ZGFhM2MyNGJjMWI5OGFmZTY1ZTVfME1ES0hJMFlGSEcyeU1kVU5CRXdaaUxZcmFXWnIxR25fVG9rZW46Ym94Y25UNnFmUlZNd3FPejgzVWlVdHJocFljXzE2Njg4NjU3OTU6MTY2ODg2OTM5NV9WNA)

æ¨¡å‹é¢„æµ‹çš„å‡†ç¡®ç‡ä¸º76%ã€‚F1å¾—åˆ†ä½çš„åŸå› æ˜¯æœ‰å…­ä¸ªç±»çš„é¢„æµ‹ï¼Œé€šè¿‡ç»“åˆâ€œTITLEâ€å’Œâ€œABSTRACTâ€æˆ–è€…åªä½¿ç”¨â€œABSTRACTâ€æ¥è®­ç»ƒå¯ä»¥æé«˜å®ƒã€‚æˆ‘å¯¹è¿™ä¸¤ä¸ªæ¡ˆä¾‹éƒ½è¿›è¡Œäº†è®­ç»ƒï¼Œå‘ç°â€œABSTRACTâ€ç‰¹å¾æœ¬èº«çš„F1åˆ†æ•°æ¯”æ ‡é¢˜å’Œæ ‡é¢˜ä¸æŠ½è±¡ç›¸ç»“åˆè¦å¥½å¾—å¤šã€‚åœ¨æ²¡æœ‰è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä½¿ç”¨æµ‹è¯•æ•°æ®è¿›è¡Œæ¨ç†ï¼Œå¹¶åœ¨private scoreä¸­è·å¾—0.82åˆ†ã€‚

æœ‰ä¸€äº›äº‹æƒ…å¯ä»¥åšï¼Œä»¥æé«˜F1æˆç»©ã€‚ä¸€ä¸ªæ˜¯å¾®è°ƒæ¨¡å‹çš„è¶…å‚æ•°ï¼Œä½ å¯èƒ½æƒ³è¦å®éªŒæ”¹å˜å­¦ä¹ é€Ÿç‡ï¼Œé€€å‡ºç‡å’Œæ—¶ä»£çš„æ•°é‡ã€‚åœ¨å¯¹æ¨¡å‹å¾®è°ƒçš„ç»“æœæ»¡æ„ä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ•´ä¸ªè®­ç»ƒæ•°æ®é›†ï¼Œè€Œä¸æ˜¯åˆ†æˆè®­ç»ƒå’ŒéªŒè¯é›†ï¼Œå› ä¸ºè®­ç»ƒæ¨¡å‹å·²ç»çœ‹åˆ°äº†æ‰€æœ‰å¯èƒ½çš„åœºæ™¯ï¼Œä½¿æ¨¡å‹æ›´å¥½åœ°æ‰§è¡Œã€‚

ä½ å¯ä»¥åœ¨è°·æ­ŒColabæŸ¥çœ‹è¿™ä¸ªé¡¹ç›®æºä»£ç 

https://colab.research.google.com/drive/1SPxxEW9okgnbMdk1ORlfSQI4rjV2tVW_#scrollTo=EJQRHd7VVMap

ä½œè€…:Kyawkhaung

åŸæ–‡åœ°å€:https://kyawkhaung.medium.com/multi-label-text-classification-with-bert-using-pytorch-47011a7313b9

deephubç¿»è¯‘ç»„

æ–‡ç« åˆ†äº«è‡ªå¾®ä¿¡å…¬ä¼—å·ï¼š

æœ¬æ–‡å‚ä¸ [è…¾è®¯äº‘è‡ªåª’ä½“åˆ†äº«è®¡åˆ’ ](https://cloud.tencent.com/developer/support-plan)ï¼Œæ¬¢è¿çƒ­çˆ±å†™ä½œçš„ä½ ä¸€èµ·å‚ä¸ï¼

å¦‚æœ‰ä¾µæƒï¼Œè¯·è”ç³»

cloudcommunity@tencent.com

åˆ é™¤ã€‚
