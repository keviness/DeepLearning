## Chapter4 神经网络的学习
### 一，从数据中学习
#### （一）数据驱动
* 深度学习有时也称为端到端机器学习（end-to-end machinelearning）。这里所说的端到端是指从一端到另一端的意思，也就是从原始数据（输入）中获得目标结果（输出）的意思。
* 神经网络的优点是对所有的问题都可以用同样的流程来解决。
#### （二）训练数据与测试数据
* 使用训练数据进行学习，寻找最优的参数；使用测试数据评价训练得到的模型的实际能力，训练数据也可以称为监督数据。
* 泛化能力是指处理未被观察过的数据（不包含在训练数据中的数据）的能力。获得泛化能力是机器学习的最终目标。
* 只对某个数据集过度拟合的状态称为过拟合（over fitting）

### 二，损失函数（Loss Function）
* 损失函数是表示神经网络性能的“恶劣程度”的指标，即当前的神经网络对监督数据在多大程度上不拟合，在多大程度上不一致。
* 以“性能的恶劣程度”为指标可能会使人感到不太自然，但是如果给损失函数乘上一个负值，就可以解释为“在多大程度上不坏”，即“性能有多好”。
* 并且，“使性能的恶劣程度达到最小”和“使性能的优良程度达到最大”是等价的，不管是用“恶劣程度”还是“优良程度”，做的事情本质上都是一样的。
* 使用训练数据进行学习，严格来说，就是针对训练数据计算损失函数的值，找出使该值尽可能小的参数
#### （一）均方误差（mean squared error）
* 公式：
![mean squared error](./imgs/MeanSquaredError.png)
* python实现：
~~~py
def mean_squared_error(y, t):
    return 0.5 * np.sum((y-t)**2)
~~~
#### （二）交叉熵误差（cross entropy error）
* 公式：
![cross entropy error](./imgs/CrossEntropyError.png)
* python实现：
~~~py
def cross_entropy_error(y, t):
    delta = 1e-7
    return -np.sum(t * np.log(y + delta))
~~~
#### （三）平均损失函数
* 把求单个数据的损失函数的式（4.2）扩大到了N份数据，不过最后还要除以N进行正规化。通过除以N，可以求单个数据的“平均损失函数”。
* 通过这样的平均化，可以获得和训练数据的数量无关的统一指标。
* 公式：
![mean loss function](./imgs/MeanLossFunction.png)