## Chapter7: 卷积神经网络（Convolutional Neural Network，CNN）
### 一，整体结构
* CNN中新出现了卷积层（Convolution层）和池化层（Pooling层）。
* 相邻层的所有神经元之间都有连接，这称为全连接（fully-connected）。
* 全连接的神经网络中，Affine层后面跟着激活函数ReLU层（或者Sigmoid层）。
* 这可以理解为之前的“Affine-ReLU”连接被替换成了“Convolution-ReLU-(Pooling)”连接。

### 二，卷积层
#### （一）全连接层存在什么问题
1. 那就是数据的形状被“忽视”了。比如，输入数据是图像时，图像通常是高、长、通道方向上的3维形状。但是，向全连接层输入时，需要将3维数据拉平为1维数据。
2. 图像是3维形状，这个形状中应该含有重要的空间信息。比如，空间上邻近的像素为相似的值、RBG的各个通道之间分别有密切的关联性、相距较远的像素之间没有什么关联等，3维形状中可能隐藏有值得提取的本质模式。但是，因为全连接层会忽视形状，将全部的输入数据作为相同的神经元（同一维度的神经元）处理，所以无法利用与形状相关的信息。
#### （二）卷积层优势
1. 卷积层可以保持形状不变。当输入数据是图像时，卷积层会以3维数据的形式接收输入数据，并同样以3维数据的形式输出至下一层。因此，在CNN中，可以（有可能）正确理解图像等具有形状的数据。
2. CNN 中，有时将卷积层的输入输出数据称为特征图（feature map）。其中，卷积层的输入数据称为输入特征图（input feature map），输出数据称为输出特征图（output feature map）。
##### 1. 卷积运算
![滤波器运算](./imgs/滤波器运算.png)
* 卷积层进行的处理就是卷积运算。卷积运算相当于图像处理中的“滤波器运算”。
* 卷积运算对输入数据应用滤波器。在这个例子中，输入数据是有高长方向的形状的数据，滤波器也一样，有高长方向上的维度。假设用（height, width）表示数据和滤波器的形状，则在本例中，输入大小是(4,4)，滤波器大小是(3, 3)，输出大小是(2, 2)。另外，有的文献中也会用“核”这个词来表示这里所说的“滤波器”。
* 对于输入数据，卷积运算以一定间隔滑动滤波器的窗口并应用。这里所说的窗口是指图7-4中灰色的3×3的部分。如图所示，将各个位置上滤波器的元素和输入的对应元素相乘，然后再求和（有时将这个计算称为乘积累加运算）。然后，将这个结果保存到输出的对应位置。将这个过程在所有位置都进行一遍，就可以得到卷积运算的输出。
* 在全连接的神经网络中，除了权重参数，还存在偏置。CNN中，滤波器的参数就对应之前的权重。并且，CNN中也存在偏置。
#### （三）填充（padding）与步幅（stride）
1. 在进行卷积层的处理之前，有时要向输入数据的周围填入固定的数据（比如0等），这称为填充（padding），是卷积运算中经常会用到的处理。
2. 通道数只能设定为和输入数据的通道数相同的值。
3. 应用滤波器的位置间隔称为步幅（stride）。
4. 当输出大小无法除尽时（结果是小数时），需要采取报错等对策。顺便说一下，根据深度学习的框架的不同，当值无法除尽时，有时会向最接近的整数四舍五入，不进行报错而继续运行。

#### （四）3维数据的卷积运算
![3DData](./imgs/3DData.png)
* 需要注意的是，在3维数据的卷积运算中，输入数据和滤波器的通道数要设为相同的值。
  
#### （五）结合方块思考
![Block Mind](./imgs/BlockMind1.png)
1. 在这个例子中，数据输出是1张特征图。所谓1张特征图，换句话说，就是通道数为1的特征图。
2. 通过应用FN个滤波器，输出特征图也生成了FN个。如果将这FN个特征图汇集在一起，就得到了形状为(FN, OH, OW)的方块。将这个方块传给下一层，就是CNN的处理流。
![Block Mind](./imgs/BlockMind2.png)
#### （六）批处理
* 神经网络的处理中进行了将输入数据打包的批处理。之前的全连接神经网络的实现也对应了批处理，通过批处理，能够实现处理的高效化和学习时对mini-batch的对应。
* 批处理版的数据流中，在各个数据的开头添加了批用的维度。像这样，数据作为4维的形状在各层间传递。这里需要注意的是，网络间传递的是4维数据，对这N个数据进行了卷积运算。也就是说，批处理将N次的处理汇总成了1次进行。
![Block Mind](./imgs/BlockMind3.png)