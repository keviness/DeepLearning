## Chapter6：与学习相关的技巧
### 一，参数的更新
* 1，最优化（optimization）：神经网络的学习的目的是找到使损失函数的值尽可能小的参数。这是寻找最优参数的问题，解决这个问题的过程称为最优化（optimization）。
* 2，随机梯度下降法（SGD）：我们将参数的梯度（导数）作为了线索。使用参数的梯度，沿梯度方向更新参数，并重复这个步骤多次，从而逐渐靠近最优参数，这个过程称为随机梯度下降法（stochastic gradient descent），简称SGD。

### 二，随机梯度下降法（SGD，stochastic gradient descent）
#### （一）python实现SGD
~~~py
class SGD:
    def __init__(self, lr=0.01):
        self.lr = lr

    def update(self, params, grads):
        for key in params.keys():
            params[key] -= self.lr * grads[key]
~~~
#### （二）SGD的缺点
* 1，如果函数的形状非均向（anisotropic），比如呈延伸状，搜索的路径就会非常低效。
* 2，SGD低效的根本原因是，梯度的方向并没有指向最小值的方向。

### 三，Momentum（动量）
