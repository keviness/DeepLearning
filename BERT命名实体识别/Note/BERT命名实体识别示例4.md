# BERTå‘½åå®ä½“è¯†åˆ«(NER)å®æˆ˜

> ğŸ”— åŸæ–‡é“¾æ¥ï¼š [https://blog.csdn.net/abc1352622216...](https://blog.csdn.net/abc13526222160/article/details/122052004)

### æ–‡ç« ç›®å½•

* ä¸€. æ•°æ®é›†ä»‹ç»
* äºŒ. æ•°æ®é›†è¯»å–&é¢„å¤„ç†
* ä¸‰. æ•°æ®åˆ†è¯tokenizer
* å››. å®šä¹‰æ•°æ®è¯»å–(ç»§æ‰¿Dataset)
* äº”. å®šä¹‰æ¨¡å‹&ä¼˜åŒ–å™¨&å­¦ä¹ ç‡
* å…­. è®­ç»ƒæµ‹è¯•ä»¥åŠå‡†ç¡®ç‡
* ä¸ƒ. æ¨¡å‹é¢„æµ‹
* å…«. æ•´ä¸ªä»£ç 
* ä¹. BILSTM+Pytorch
* å. å‚è€ƒ
* **BERTæŠ€æœ¯è¯¦ç»†ä»‹ç»ï¼š **[https://zhangkaifang.blog.csdn.net/article/details/120507302](https://zhangkaifang.blog.csdn.net/article/details/120507302)
* **æœ¬é¡¹ç›®ä»£ç githubé“¾æ¥ï¼š **[https://github.com/zhangkaifang/NLP-Learning](https://github.com/zhangkaifang/NLP-Learning)
* **BERTå‘½åå®ä½“è¯†åˆ«æ¨¡å‹å¦‚ä¸‹ï¼š**

![](https://fjjwhjwd3p.feishu.cn/space/api/box/stream/download/asynccode/?code=ZmFhMjRiYWJjZjYyZjBkNmJkYzRkMThjNjE3OTYyNDJfdkxrOWV1TnAzRXZNZG9USnVndTE2QUZwOE5Va2t5cVlfVG9rZW46Ym94Y25HNmRWRWxRNkdING04Y1B0UUdoT2NkXzE2Njk1NDAyNjU6MTY2OTU0Mzg2NV9WNA)

# ä¸€. æ•°æ®é›†ä»‹ç»

* **å®éªŒä½¿ç”¨çš„æ•°æ®é›†æ˜¯å¾®è½¯äºšæ´²ç ”ç©¶é™¢æä¾›çš„è¯æ€§æ ‡æ³¨æ•°æ®é›†ï¼Œå…¶ç›®æ ‡æ˜¯è¯†åˆ«æ–‡æœ¬ä¸­å…·æœ‰ç‰¹å®šæ„ä¹‰çš„å®ä½“,åŒ…æ‹¬äººåã€åœ°åã€æœºæ„åã€‚é“¾æ¥ï¼š **[https://mirror.coggle.club/dataset/ner/msra.zip](https://mirror.coggle.club/dataset/ner/msra.zip)
* **ç™¾åº¦äº‘é“¾æ¥: **[https://pan.baidu.com/s/17MRMTrQKWJ6-HUI-rWL80A ](https://pan.baidu.com/s/17MRMTrQKWJ6-HUI-rWL80A)æå–ç : oq7w

![](https://fjjwhjwd3p.feishu.cn/space/api/box/stream/download/asynccode/?code=NGMyY2QxMTI1YWM3M2VhZDNiOTQxOWJkZGFhZjlhZGJfb21ob2RUY0htcHZYOFpkc09tMEhvU1ZUUjVzMWdDRmhfVG9rZW46Ym94Y25sWnVCbjBOUTlKYWhDVDNtV1FXa05oXzE2Njk1NDAyNjU6MTY2OTU0Mzg2NV9WNA)

![](https://fjjwhjwd3p.feishu.cn/space/api/box/stream/download/asynccode/?code=OTQzZDNhYzY0YjlkYmQyODgwZWMyNWQ1Njg5MjU4ZDBfQ2VSUEJpaEtDQmtHYnFzendMNHo2eUlPcUs1a0Jwd3dfVG9rZW46Ym94Y25NcU91QmpVa3VFVEoyR2RSME1taW1lXzE2Njk1NDAyNjU6MTY2OTU0Mzg2NV9WNA)

# äºŒ. æ•°æ®é›†è¯»å–&é¢„å¤„ç†

```Python
import codecs

################## 1. è¯»å–æ•°æ®
# è®­ç»ƒæ•°æ®å’Œæ ‡ç­¾
train_lines = codecs.open('msra/train/sentences.txt').readlines()
train_lines = [x.replace(' ', '').strip() for x in train_lines]  # ç”¨äºç§»é™¤å­—ç¬¦ä¸²å¼€å¤´å’Œç»“å°¾æŒ‡å®šçš„å­—ç¬¦ï¼ˆé»˜è®¤ä¸ºç©ºæ ¼æˆ–æ¢è¡Œç¬¦ï¼‰æˆ–å­—ç¬¦åºåˆ—ã€‚
train_tags = codecs.open('msra/train/tags.txt').readlines()
train_tags = [x.strip().split(' ') for x in train_tags]
train_tags = [[tag_type.index(x) for x in tag] for tag in train_tags]
train_lines, train_tags = train_lines[:20000], train_tags[:20000]  # åªå–ä¸¤ä¸‡æ•°æ®
print(train_lines[0], "\n", train_tags[0])
# å¦‚ä½•è§£å†³è¶³çƒç•Œé•¿æœŸå­˜åœ¨çš„è¯¸å¤šçŸ›ç›¾ï¼Œé‡æŒ¯æ˜”æ—¥æ´¥é—¨è¶³çƒçš„é›„é£ï¼Œæˆä¸ºå¤©æ´¥è¶³å›ä¸Šä¸‹å†…å¤–åˆ°å¤„è®®è®ºçš„è¯é¢˜ã€‚ 
# [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]

# éªŒè¯æ•°æ®å’Œæ ‡ç­¾
val_lines = codecs.open('msra/val/sentences.txt').readlines()
val_lines = [x.replace(' ', '').strip() for x in val_lines]
val_tags = codecs.open('msra/val/tags.txt').readlines()
val_tags = [x.strip().split(' ') for x in val_tags]
val_tags = [[tag_type.index(x) for x in tag] for tag in val_tags]
```

# ä¸‰. æ•°æ®åˆ†è¯tokenizer

* **æ³¨æ„ï¼šä¸­æ–‡æ³¨æ„åŠ  ****`list(train_lines)`**** ,å› ä¸ºä¸åŠ å› ä¸ºå•è¯ä½œä¸ºæ•´ä½“äº†ã€‚**

```Python
################## 2. å¯¹æ•°æ®è¿›è¡Œåˆ†è¯
tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
# ä¸­æ–‡æ³¨æ„åŠ list(train_lines),å› ä¸ºä¸åŠ å› ä¸ºå•è¯ä½œä¸ºæ•´ä½“äº†ã€‚
max_length = 64
train_encoding = tokenizer.batch_encode_plus(list(train_lines), truncation=True, padding=True, max_length=max_length)
val_encoding = tokenizer.batch_encode_plus(list(val_lines), truncation=True, padding=True, max_length=max_length)
```

# å››. å®šä¹‰æ•°æ®è¯»å–(ç»§æ‰¿Dataset)

* **æ³¨æ„ï¼šä¸‹é¢labelséœ€è¦å¡«å……å¼€å¤´clsï¼Œç»“å°¾éƒ¨åˆ†ä¸å¤Ÿmaxlenä¹Ÿè¦å¡«0ã€‚**

```Python
################## 3. å®šä¹‰Datasetç±»å¯¹è±¡
class TextDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(value[idx][:maxlen]) for key, value in self.encodings.items()}
        # å­—çº§åˆ«çš„æ ‡æ³¨ï¼Œæ³¨æ„å¡«å……clsï¼Œè¿™é‡Œ[0]ä»£è¡¨clsã€‚åé¢ä¸å¤Ÿé•¿çš„è¿™é‡Œä¹Ÿæ˜¯è¡¥å……0ï¼Œæ ·æœ¬tokenizerçš„æ—¶å€™å·²ç»å¡«å……äº†
        # item['labels'] = torch.tensor([0] + self.labels[idx] + [0] * (63-len(self.labels[idx])))[:64]
        item['labels'] = torch.tensor([0] + self.labels[idx] + [0] * (maxlen - 1 - len(self.labels[idx])))[:maxlen]
        return item

    def __len__(self):
        return len(self.labels)


train_dataset = TextDataset(train_encoding, train_tags)
test_dataset = TextDataset(val_encoding, val_tags)
print(train_dataset[0])

# Datasetè½¬æ¢æˆDataloader
batchsz = 32
train_loader = DataLoader(train_dataset, batch_size=batchsz, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batchsz, shuffle=True)
```

# äº”. å®šä¹‰æ¨¡å‹&ä¼˜åŒ–å™¨&å­¦ä¹ ç‡

```Python
from transformers import BertForTokenClassification, AdamW, get_linear_schedule_with_warmup

################## 4. å®šä¹‰æ¨¡å‹
model = BertForTokenClassification.from_pretrained('bert-base-chinese', num_labels=7)
device = torch.device("cuda:2" if torch.cuda.is_available() else "cpu")
model.to(device)

# ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡
optimizer = AdamW(model.parameters(), lr=5e-5)
total_steps = len(train_loader) * 1
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0,
                                            num_training_steps=total_steps)  # Default value in run_glue.py
```

# å…­. è®­ç»ƒæµ‹è¯•ä»¥åŠå‡†ç¡®ç‡

* **æ³¨æ„ï¼šoutputsè¾“å‡ºç»“æœä¸­çš„logitsï¼ŒNERå…¶å®å°±æ˜¯å¯¹æ¯ä¸ªtokenè¿›è¡Œåˆ†ç±»ã€‚**

![](https://fjjwhjwd3p.feishu.cn/space/api/box/stream/download/asynccode/?code=NjgyYmUwZTAxMTFlNGIwMjg2OWNjNzAzNjIwODE5MWFfTGxyS29NWm1rRERGMFE3MWxrdnJTZTdnTWVxVEdLVE5fVG9rZW46Ym94Y25OVFYyZzQzbGlSUzlLaGhhVTZONGliXzE2Njk1NDAyNjU6MTY2OTU0Mzg2NV9WNA)

* **ç„¶åå¯¹dim=2ç»´åº¦ä¸Šå–argmaxï¼Œæ‰¾å‡ºæ¯ä¸ªä½ç½®æ‰€å±çš„ç±»åˆ«ä¸‹æ ‡ã€‚**

![](https://fjjwhjwd3p.feishu.cn/space/api/box/stream/download/asynccode/?code=OWY0NzhhZWQwMjcxMzJmZDk3YzIzMTAxZTk2YTZiMzVfaU5pN2RnREFsY2FqN2JDOGd0RTdwOWtKdHNSSFd6RlNfVG9rZW46Ym94Y25yemdDbFpLcXB6VzkybEJuOEl3VHpmXzE2Njk1NDAyNjU6MTY2OTU0Mzg2NV9WNA)

```Shell
# è¿™é‡Œæµ‹è¯•è®¡ç®—å‡†ç¡®ç‡ä¸­çš„ï¼š
a = torch.tensor([1, 2, 3, 4, 2])
b = torch.tensor([1, 2, 4, 3, 2])
print((a==b).float().mean())
print((a==b).float().mean().item())
```

* **ä»£ç å¦‚ä¸‹ï¼š**

```Python
from tqdm import tqdm

def train():
    model.train()
    total_train_loss = 0
    iter_num = 0
    total_iter = len(train_loader)
    for idx, batch in enumerate(train_loader):
        optim.zero_grad()
      
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)
        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
            # loss = outputs[0]

        loss = outputs.loss
      
        if idx % 20 == 0:
            with torch.no_grad():
                # 64 * 7
                print((outputs[1].argmax(2).data == labels.data).float().mean().item(), loss.item())
      
        total_train_loss += loss.item()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optim.step()
        scheduler.step()

        iter_num += 1
        if(iter_num % 100==0):
            print("epoth: %d, iter_num: %d, loss: %.4f, %.2f%%" % (epoch, iter_num, loss.item(), iter_num/total_iter*100))
      
    print("Epoch: %d, Average training loss: %.4f"%(epoch, total_train_loss/len(train_loader)))
  
def validation():
    model.eval()
    total_eval_accuracy = 0
    total_eval_loss = 0
    for batch in test_dataloader:
        with torch.no_grad():
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        logits = outputs[1]

        total_eval_loss += loss.item()
        logits = logits.detach().cpu().numpy()
        label_ids = labels.to('cpu').numpy()
        total_eval_accuracy += (outputs[1].argmax(2).data == labels.data).float().mean().item()
      
    avg_val_accuracy = total_eval_accuracy / len(test_dataloader)
    print("Accuracy: %.4f" % (avg_val_accuracy))
    print("Average testing loss: %.4f"%(total_eval_loss/len(test_dataloader)))
    print("-------------------------------")
  

for epoch in range(4):
    print("------------Epoch: %d ----------------" % epoch)
    train()
    validation()
```

# ä¸ƒ. æ¨¡å‹é¢„æµ‹

```Python
model = torch.load('bert-ner.pt')

tag_type = ['O', 'B-ORG', 'I-ORG', 'B-PER', 'I-PER', 'B-LOC', 'I-LOC']

def predcit(s):
    item = tokenizer([s], truncation=True, padding='longest', max_length=64) # åŠ ä¸€ä¸ªlist
    with torch.no_grad():
        input_ids = torch.tensor(item['input_ids']).to(device).reshape(1, -1)
        attention_mask = torch.tensor(item['attention_mask']).to(device).reshape(1, -1)
        labels = torch.tensor([0] * attention_mask.shape[1]).to(device).reshape(1, -1)
      
        outputs = model(input_ids, attention_mask, labels)
        outputs = outputs[0].data.cpu().numpy()
      
    outputs = outputs[0].argmax(1)[1:-1]
    ner_result = ''
    ner_flag = ''
  
    for o, c in zip(outputs,s):
        # 0 å°±æ˜¯ Oï¼Œæ²¡æœ‰å«ä¹‰
        if o == 0 and ner_result == '':
            continue
      
        # 
        elif o == 0 and ner_result != '':
            if ner_flag == 'O':
                print('æœºæ„ï¼š', ner_result)
            if ner_flag == 'P':
                print('äººåï¼š', ner_result)
            if ner_flag == 'L':
                print('ä½ç½®ï¼š', ner_result)
              
            ner_result = ''
      
        elif o != 0:
            ner_flag = tag_type[o][2]
            ner_result += c
    return outputs

s = 'æ•´ä¸ªåç››é¡¿å·²ç¬¼ç½©åœ¨ä¸€ç‰‡å¤œè‰²ä¹‹ä¸­ï¼Œä¸€ä¸ªç”µè¯ä»ç¾å›½æ€»ç»Ÿåºœç™½å®«æ‰“åˆ°äº†è²å¾‹å®¾æ€»ç»Ÿåºœé©¬æ‹‰å¡å—å®«ã€‚'
# è¯†åˆ«å‡ºå¥å­é‡Œé¢çš„å®ä½“è¯†åˆ«ï¼ˆNERï¼‰
data = predcit(s)
s = 'äººå·¥æ™ºèƒ½æ˜¯æœªæ¥çš„å¸Œæœ›ï¼Œä¹Ÿæ˜¯ä¸­å›½å’Œç¾å›½çš„å†²çªç‚¹ã€‚'
data = predcit(s)
s = 'æ˜å¤©æˆ‘ä»¬ä¸€èµ·åœ¨æµ·æ·€åƒä¸ªé¥­å§ï¼ŒæŠŠå«åˆ˜æ¶›å’Œç‹åä¹Ÿå«ä¸Šã€‚'
data = predcit(s)
s = 'åŒç…¤é›†å›¢åŒç”Ÿå®‰å¹³ç…¤ä¸šå…¬å¸å‘ç”Ÿäº•ä¸‹å®‰å…¨äº‹æ•… 19åçŸ¿å·¥é‡éš¾'
data = predcit(s)
s = 'å±±ä¸œçœæ”¿åºœåŠå…¬å…å°±å¹³é‚‘å¿ç‰è£å•†è´¸æœ‰é™å…¬å¸çŸ³è†çŸ¿åå¡Œäº‹æ•…å‘å‡ºé€šæŠ¥'
data = predcit(s)
s = '[æ–°é—»ç›´æ’­é—´]é»‘é¾™æ±Ÿ:é¾™ç…¤é›†å›¢ä¸€ç…¤çŸ¿å‘ç”Ÿç«ç¾äº‹æ•…'
data = predcit(s)
```

```Shell
ä½ç½®ï¼š åç››é¡¿
ä½ç½®ï¼š ç¾å›½æ€»ç»Ÿåºœç™½å®«
ä½ç½®ï¼š è²å¾‹å®¾æ€»ç»Ÿåºœé©¬æ‹‰å¡å—å®«
ä½ç½®ï¼š åç››é¡¿
ä½ç½®ï¼š ç¾å›½æ€»ç»Ÿåºœç™½å®«
ä½ç½®ï¼š è²å¾‹å®¾æ€»ç»Ÿåºœé©¬æ‹‰å¡å—å®«
ä½ç½®ï¼š ä¸­å›½
ä½ç½®ï¼š ç¾å›½
ä½ç½®ï¼š æµ·æ·€
äººåï¼š åˆ˜æ¶›
äººåï¼š ç‹å
æœºæ„ï¼š åŒç…¤é›†å›¢åŒç”Ÿå®‰å¹³ç…¤ä¸šå…¬å¸
æœºæ„ï¼š å±±ä¸œçœæ”¿åºœåŠå…¬å…
æœºæ„ï¼š å¹³é‚‘å¿ç‰è£å•†è´¸æœ‰é™å…¬å¸
ä½ç½®ï¼š é»‘é¾™æ±Ÿ
æœºæ„ï¼š é¾™ç…¤é›†å›¢
```

# å…«. æ•´ä¸ªä»£ç 

* **æ­¤å¤–æä¾›äº†notebookç‰ˆæœ¬ä»£ç ï¼Œç™¾åº¦äº‘: **[https://pan.baidu.com/s/1tiLqvsdzuBgWFb6defNyWg ](https://pan.baidu.com/s/1tiLqvsdzuBgWFb6defNyWg)æå–ç : gu8t

```Python
# !/usr/bin/env python
# -*- encoding: utf-8 -*-
"""=====================================
@author : kaifang zhang
@time   : 2021/12/19 1:33 PM
@contact: kaifang.zkf@dtwave-inc.com
====================================="""
import codecs
import torch
from tqdm import tqdm
from torch.utils.data import Dataset, DataLoader, TensorDataset
from transformers import BertTokenizer
from transformers import BertForTokenClassification, AdamW, get_linear_schedule_with_warmup

tag_type = ['O', 'B-ORG', 'I-ORG', 'B-PER', 'I-PER', 'B-LOC', 'I-LOC']
# B-ORG I-ORG æœºæ„çš„å¼€å§‹ä½ç½®å’Œä¸­é—´ä½ç½®
# B-PER I-PER äººç‰©åå­—çš„å¼€å§‹ä½ç½®å’Œä¸­é—´ä½ç½®
# B-LOC I-LOC ä½ç½®çš„å¼€å§‹ä½ç½®å’Œä¸­é—´ä½ç½®

################## 1. è¯»å–æ•°æ®
# è®­ç»ƒæ•°æ®å’Œæ ‡ç­¾
train_lines = codecs.open('msra/train/sentences.txt').readlines()
train_lines = [x.replace(' ', '').strip() for x in train_lines]  # ç”¨äºç§»é™¤å­—ç¬¦ä¸²å¼€å¤´å’Œç»“å°¾æŒ‡å®šçš„å­—ç¬¦ï¼ˆé»˜è®¤ä¸ºç©ºæ ¼æˆ–æ¢è¡Œç¬¦ï¼‰æˆ–å­—ç¬¦åºåˆ—ã€‚
train_tags = codecs.open('msra/train/tags.txt').readlines()
train_tags = [x.strip().split(' ') for x in train_tags]
train_tags = [[tag_type.index(x) for x in tag] for tag in train_tags]
train_lines, train_tags = train_lines[:20000], train_tags[:20000]  # åªå–ä¸¤ä¸‡æ•°æ®
print(f"æ ·ä¾‹æ•°æ®ï¼š{train_lines[0]} \næ ·ä¾‹æ ‡ç­¾ï¼š{train_tags[0]}")

# éªŒè¯æ•°æ®å’Œæ ‡ç­¾
val_lines = codecs.open('msra/val/sentences.txt').readlines()
val_lines = [x.replace(' ', '').strip() for x in val_lines]
val_tags = codecs.open('msra/val/tags.txt').readlines()
val_tags = [x.strip().split(' ') for x in val_tags]
val_tags = [[tag_type.index(x) for x in tag] for tag in val_tags]  # æ ‡ç­¾è½¬æ¢ä¸ºæ•°å€¼

################## 2. å¯¹æ•°æ®è¿›è¡Œåˆ†è¯
tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
# ä¸­æ–‡æ³¨æ„åŠ list(train_lines),å› ä¸ºä¸åŠ å› ä¸ºå•è¯ä½œä¸ºæ•´ä½“äº†ã€‚
maxlen = 64
train_encoding = tokenizer.batch_encode_plus(list(train_lines), truncation=True, padding=True, max_length=maxlen)
val_encoding = tokenizer.batch_encode_plus(list(val_lines), truncation=True, padding=True, max_length=maxlen)

################## 3. å®šä¹‰Datasetç±»å¯¹è±¡
class TextDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(value[idx][:maxlen]) for key, value in self.encodings.items()}
        # å­—çº§åˆ«çš„æ ‡æ³¨ï¼Œæ³¨æ„å¡«å……clsï¼Œè¿™é‡Œ[0]ä»£è¡¨clsã€‚åé¢ä¸å¤Ÿé•¿çš„è¿™é‡Œä¹Ÿæ˜¯è¡¥å……0ï¼Œæ ·æœ¬tokenizerçš„æ—¶å€™å·²ç»å¡«å……äº†
        # item['labels'] = torch.tensor([0] + self.labels[idx] + [0] * (63-len(self.labels[idx])))[:64]
        item['labels'] = torch.tensor([0] + self.labels[idx] + [0] * (maxlen - 1 - len(self.labels[idx])))[:maxlen]
        return item

    def __len__(self):
        return len(self.labels)

train_dataset = TextDataset(train_encoding, train_tags)
test_dataset = TextDataset(val_encoding, val_tags)
batchsz = 32
train_loader = DataLoader(train_dataset, batch_size=batchsz, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batchsz, shuffle=True)

# print(train_dataset[0])

# æµ‹è¯•æ ·æœ¬æ˜¯å¦æ»¡è¶³æœ€å¤§é•¿åº¦
for idx in range(len(train_dataset)):
    item = train_dataset[idx]
    for key in item:
        if item[key].shape[0] != 64:
            print(key, item[key].shape)
for idx in range(len(test_dataset)):
    item = test_dataset[idx]
    for key in item:
        if item[key].shape[0] != 64:
            print(key, item[key].shape)

################## 4. å®šä¹‰æ¨¡å‹
model = BertForTokenClassification.from_pretrained('bert-base-chinese', num_labels=7)
device = torch.device("cuda:1" if torch.cuda.is_available() else "cpu")
model.to(device)

# ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡
optimizer = AdamW(model.parameters(), lr=5e-5)
total_steps = len(train_loader) * 1
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0,
                                            num_training_steps=total_steps)  # Default value in run_glue.py

################## 4. è®­ç»ƒæµ‹è¯•ä»¥åŠå­—ç¬¦çš„åˆ†ç±»å‡†ç¡®ç‡
def train():
    model.train()
    total_train_loss = 0
    iter_num = 0
    total_iter = len(train_loader)
    for idx, batch in enumerate(train_loader):
        optimizer.zero_grad()

        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)  # shape: [32, 64]
        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs[0]
        # loss = outputs.loss
        logits1 = outputs[1]  # shape: [32, 64, 7]
        out = logits1.argmax(dim=2)
        out1 = out.data
        # logits2 = outputs.logits

        if idx % 20 == 0:  # çœ‹æ¨¡å‹çš„å‡†ç¡®ç‡
            with torch.no_grad():
                # å‡å¦‚è¾“å…¥çš„æ˜¯64ä¸ªå­—ç¬¦ï¼Œ64 * 7
                print((outputs[1].argmax(2).data == labels.data).float().mean().item(), loss.item())

        total_train_loss += loss.item()

        # åå‘æ¢¯åº¦ä¿¡æ¯
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

        # å‚æ•°æ›´æ–°
        optimizer.step()
        scheduler.step()

        iter_num += 1
        if (iter_num % 100 == 0):
            print("epoth: %d, iter_num: %d, loss: %.4f, %.2f%%" % (
                epoch, iter_num, loss.item(), iter_num / total_iter * 100))

    print("Epoch: %d, Average training loss: %.4f" % (epoch, total_train_loss / len(train_loader)))

def validation():
    model.eval()
    total_eval_accuracy = 0
    total_eval_loss = 0
    for batch in test_loader:
        with torch.no_grad():
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        logits = outputs[1]

        total_eval_loss += loss.item()
        logits = logits.detach().cpu().numpy()
        label_ids = labels.to('cpu').numpy()
        total_eval_accuracy += (outputs[1].argmax(2).data == labels.data).float().mean().item()

    avg_val_accuracy = total_eval_accuracy / len(test_loader)
    print("Accuracy: %.4f" % (avg_val_accuracy))
    print("Average testing loss: %.4f" % (total_eval_loss / len(test_loader)))
    print("-------------------------------")

# tag_type = ['O', 'B-ORG', 'I-ORG', 'B-PER', 'I-PER', 'B-LOC', 'I-LOC']
def predcit(s):
    item = tokenizer([s], truncation=True, padding='longest', max_length=64)  # åŠ ä¸€ä¸ªlist
    with torch.no_grad():
        input_ids = torch.tensor(item['input_ids']).to(device).reshape(1, -1)
        attention_mask = torch.tensor(item['attention_mask']).to(device).reshape(1, -1)
        labels = torch.tensor([0] * attention_mask.shape[1]).to(device).reshape(1, -1)

        outputs = model(input_ids, attention_mask, labels)
        outputs = outputs[0].data.cpu().numpy()

    outputs = outputs[0].argmax(1)[1:-1]
    ner_result = ''
    ner_flag = ''

    for o, c in zip(outputs, s):
        # 0 å°±æ˜¯ Oï¼Œæ²¡æœ‰å«ä¹‰
        if o == 0 and ner_result == '':
            continue
        #
        elif o == 0 and ner_result != '':
            if ner_flag == 'O':
                print('æœºæ„ï¼š', ner_result)
            if ner_flag == 'P':
                print('äººåï¼š', ner_result)
            if ner_flag == 'L':
                print('ä½ç½®ï¼š', ner_result)

            ner_result = ''

        elif o != 0:
            ner_flag = tag_type[o][2]
            ner_result += c
    return outputs

# for epoch in range(4):
#     print("------------Epoch: %d ----------------" % epoch)
#     train()
#     validation()
# torch.save(model, 'bert-ner.pt')

model = torch.load('/data/aibox/kaifang/NLPå­¦ä¹ èµ„æ–™/bert-ner.pt')
s = 'æ•´ä¸ªåç››é¡¿å·²ç¬¼ç½©åœ¨ä¸€ç‰‡å¤œè‰²ä¹‹ä¸­ï¼Œä¸€ä¸ªç”µè¯ä»ç¾å›½æ€»ç»Ÿåºœç™½å®«æ‰“åˆ°äº†è²å¾‹å®¾æ€»ç»Ÿåºœé©¬æ‹‰å¡å—å®«ã€‚'
# è¯†åˆ«å‡ºå¥å­é‡Œé¢çš„å®ä½“è¯†åˆ«ï¼ˆNERï¼‰
data = predcit(s)
s = 'æ•´ä¸ªåç››é¡¿å·²ç¬¼ç½©åœ¨ä¸€ç‰‡å¤œè‰²ä¹‹ä¸­ï¼Œä¸€ä¸ªç”µè¯ä»ç¾å›½æ€»ç»Ÿåºœç™½å®«æ‰“åˆ°äº†è²å¾‹å®¾æ€»ç»Ÿåºœé©¬æ‹‰å¡å—å®«ã€‚'
# è¯†åˆ«å‡ºå¥å­é‡Œé¢çš„å®ä½“è¯†åˆ«ï¼ˆNERï¼‰
data = predcit(s)
s = 'äººå·¥æ™ºèƒ½æ˜¯æœªæ¥çš„å¸Œæœ›ï¼Œä¹Ÿæ˜¯ä¸­å›½å’Œç¾å›½çš„å†²çªç‚¹ã€‚'
data = predcit(s)
s = 'æ˜å¤©æˆ‘ä»¬ä¸€èµ·åœ¨æµ·æ·€åƒä¸ªé¥­å§ï¼ŒæŠŠå«åˆ˜æ¶›å’Œç‹åä¹Ÿå«ä¸Šã€‚'
data = predcit(s)
s = 'åŒç…¤é›†å›¢åŒç”Ÿå®‰å¹³ç…¤ä¸šå…¬å¸å‘ç”Ÿäº•ä¸‹å®‰å…¨äº‹æ•… 19åçŸ¿å·¥é‡éš¾'
data = predcit(s)
s = 'å±±ä¸œçœæ”¿åºœåŠå…¬å…å°±å¹³é‚‘å¿ç‰è£å•†è´¸æœ‰é™å…¬å¸çŸ³è†çŸ¿åå¡Œäº‹æ•…å‘å‡ºé€šæŠ¥'
data = predcit(s)
s = '[æ–°é—»ç›´æ’­é—´]é»‘é¾™æ±Ÿ:é¾™ç…¤é›†å›¢ä¸€ç…¤çŸ¿å‘ç”Ÿç«ç¾äº‹æ•…'
data = predcit(s)
```

# ä¹. BILSTM+Pytorch

* **ä»£ç æ•°æ®ä¸‹è½½é“¾æ¥ ** ï¼š [https://www.aliyundrive.com/s/oQJFwaSt17p](https://www.aliyundrive.com/s/oQJFwaSt17p)

```PowerShell
# !/usr/bin/env python
# -*- encoding: utf-8 -*-
"""=====================================
@author : kaifang zhang
@time   : 2022/2/1 23:45
@contact: kaifang.zkf@dtwave-inc.com
====================================="""
import os
import tqdm
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import f1_score


def build_corpus(split, make_vocab=True, data_dir="data"):
    """ è¯»å–æ•°æ® """
    assert split in ["train", "dev", "test"]
    word_lists, tag_lists = [], []
    with open(os.path.join(data_dir, split + ".char.bmes"), mode="r", encoding="utf-8") as f:
        word_list, tag_list = [], []
        for line in f:
            if line != "\n":
                word, tag = line.strip("\n").split()
                word_list.append(word)
                tag_list.append(tag)
            else:
                word_lists.append(word_list)
                tag_lists.append(tag_list)
                word_list, tag_list = [], []
    word_lists = sorted(word_lists, key=lambda x: len(x), reverse=False)
    tag_lists = sorted(tag_lists, key=lambda x: len(x), reverse=False)

    # å¦‚æœmake_vocabä¸ºTrueï¼Œè¿˜éœ€è¦è¿”å›word2idå’Œtag2id
    if make_vocab:
        word2id = build_map(word_lists)
        tag2id = build_map(tag_lists)
        word2id['<UNK>'] = len(word2id)
        word2id['<PAD>'] = len(word2id)
        tag2id['<PAD>'] = len(tag2id)
        return word_lists, tag_lists, word2id, tag2id
    else:
        return word_lists, tag_lists


def build_map(lists):
    maps = {}
    for list_ in lists:
        for e in list_:
            if e not in maps:
                maps[e] = len(maps)
    return maps


class MyDataset(nn.Module):
    """ è‡ªå®šä¹‰Datasetç±» """

    def __init__(self, datas, tags, word2index, tag2index):
        self.datas = datas
        self.tags = tags
        self.word2index = word2index
        self.tag2index = tag2index

    def __getitem__(self, index):
        data = self.datas[index]
        tag = self.tags[index]

        data_index = [self.word2index.get(i, self.word2index['<UNK>']) for i in data]
        tag_index = [self.tag2index[i] for i in tag]

        return data_index, tag_index

    def __len__(self):
        assert len(self.datas) == len(self.tags)
        return len(self.tags)

    def pro_batch_data(self, batch_datas):
        """ æ¯ä¸ªbatchå¦‚ä½•è‡ªåŠ¨å¡«å…… """
        global device
        datas, tags, batch_lens = [], [], []
        for data, tag in batch_datas:
            datas.append(data)
            tags.append(tag)
            batch_lens.append(len(data))
        batch_max_len = max(batch_lens)
        datas = [i + [self.word2index['<PAD>']] * (batch_max_len - len(i)) for i in datas]
        tags = [i + [self.tag2index['<PAD>']] * (batch_max_len - len(i)) for i in tags]

        return torch.tensor(datas, dtype=torch.int64, device=device), torch.tensor(tags, dtype=torch.long,
                                                                                   device=device)  # longä¹Ÿæ˜¯int64


class MyModel(nn.Module):
    def __init__(self, corpus_num, embedding_num, hidden_num, class_num, bi=True):
        super().__init__()
        self.embedding = nn.Embedding(corpus_num, embedding_num)
        self.lstm = nn.LSTM(embedding_num, hidden_num, batch_first=True, bidirectional=bi)

        if bi:
            self.classifer = nn.Linear(hidden_num * 2, class_num)
        else:
            self.classifer = nn.Linear(hidden_num, class_num)
        self.cross_loss = nn.CrossEntropyLoss()

    def forward(self, batch_data, batch_tag=None):
        embedding = self.embedding(batch_data)
        out, _ = self.lstm(embedding)
        pred = self.classifer(out)
        self.pred = torch.argmax(pred, dim=-1).reshape(-1)
        if batch_tag is not None:
            loss = self.cross_loss(pred.reshape(-1, pred.shape[-1]), batch_tag.reshape(-1))
            return loss


def test():
    global word2index, model, index2tag, device  # å…¨å±€å˜é‡å£°æ˜ï¼Œåªæ˜¯è¯»å–
    while True:
        text = input("è¯·è¾“å…¥ï¼š")
        text_index = [[word2index.get(i, word2index['<UNK>']) for i in text]]
        text_index = torch.tensor(text_index, dtype=torch.int64, device=device)
        model.forward(text_index)
        pred = [index2tag[i] for i in model.pred]

        print([f'{w}_{s}' for w, s in zip(text, pred)])


if __name__ == '__main__':
    device = "cuda:0" if torch.cuda.is_available() else "cpu"

    train_word_lists, train_tag_lists, word2index, tag2index = build_corpus("train", make_vocab=True)
    dev_data, dev_tag = build_corpus("dev", make_vocab=False)
    index2tag = [i for i in tag2index]

    # å®šä¹‰ä¸€äº›å˜é‡
    corpus_num = len(word2index)
    class_num = len(tag2index)  # å‘½åå®ä½“è¯†åˆ«å°±æ˜¯ä¸ºæ¯ä¸ªå­—è¿›è¡Œåˆ†ç±»
    epoch = 50
    lr = 0.001
    embedding = 101
    hidden_num = 107
    bi = True
    batchsz = 64

    train_dataset = MyDataset(train_word_lists, train_tag_lists, word2index, tag2index)
    # è‡ªå·±å¤„ç†ï¼šcollate_fn=train_dataset.pro_batch_data
    train_dataloader = DataLoader(train_dataset, batch_size=batchsz, shuffle=False,
                                  collate_fn=train_dataset.pro_batch_data)

    dev_dataset = MyDataset(dev_data, dev_tag, word2index, tag2index)
    dev_dataloader = DataLoader(dev_dataset, batch_size=batchsz, shuffle=False,
                                collate_fn=dev_dataset.pro_batch_data)

    model = MyModel(corpus_num, embedding, hidden_num, class_num, bi)
    opt = torch.optim.Adam(model.parameters(), lr=lr)
    model = model.to(device)

    for e in tqdm.trange(epoch):
        model.train()
        for batch_data, batch_tag in train_dataloader:
            train_loss = model(batch_data, batch_tag)
            train_loss.backward()
            opt.step()
            opt.zero_grad()
        print(f"train loss: {train_loss:.3f}")

        model.eval()
        all_pred, all_tag = [], []
        for dev_batch_data, dev_batch_tag in dev_dataloader:
            dev_loss = model(dev_batch_data, dev_batch_tag)
            all_pred.extend(model.pred.detach().cpu().numpy().tolist())
            all_tag.extend(dev_batch_tag.detach().cpu().numpy().reshape(-1).tolist())
        # print(f"dev loss: {dev_loss:.3f}")
        score = f1_score(all_tag, all_pred, average="macro")
        print(f"{e},f1_score:{score:.3f},dev_loss:{dev_loss:.3f}")
    # test()
```

# å. å‚è€ƒ

* **ä¸»è¦å‚è€ƒdasouåšä¸»çš„è§†é¢‘ ** ï¼š [https://www.bilibili.com/video/BV1Ey4y1874y?p=6&amp;spm_id_from=pageDriver](https://www.bilibili.com/video/BV1Ey4y1874y?p=6&spm_id_from=pageDriver)
* **è…¾è®¯Buglyçš„ä¸“æ  ** ï¼š [å›¾è§£BERTæ¨¡å‹ï¼šä»é›¶å¼€å§‹æ„å»ºBERT](https://cloud.tencent.com/developer/article/1389555)
* Bertæºä»£ç è§£è¯»-ä»¥BERTæ–‡æœ¬åˆ†ç±»ä»£ç ä¸ºä¾‹å­ï¼š [https://github.com/DA-southampton/Read_Bert_Code](https://github.com/DA-southampton/Read_Bert_Code)
* BERTå¤§ç«å´ä¸æ‡‚Transformerï¼Ÿè¯»è¿™ä¸€ç¯‡å°±å¤Ÿäº†ï¼š [https://zhuanlan.zhihu.com/p/54356280](https://zhuanlan.zhihu.com/p/54356280)
* pytorch ä¸­åŠ è½½BERTæ¨¡å‹, è·å–è¯å‘é‡ï¼š [https://blog.csdn.net/znsoft/article/details/107725285](https://blog.csdn.net/znsoft/article/details/107725285)
* [Bertç”Ÿæˆå¥å‘é‡(pytorch)](https://blog.csdn.net/weixin_30034903/article/details/113399809?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_title~default-1.no_search_link&spm=1001.2101.3001.4242.1)
* [https://blog.csdn.net/weixin_41519463/article/details/100863313](https://blog.csdn.net/weixin_41519463/article/details/100863313)
* å­¦ä¹ ç‡é¢„çƒ­(transformers.get_linear_schedule_with_warmup)ï¼š [https://blog.csdn.net/orangerfun/article/details/120400247](https://blog.csdn.net/orangerfun/article/details/120400247)
