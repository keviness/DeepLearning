## 模型实验

采用[sentence-transformer](https://link.zhihu.com/?target=https%3A//github.com/UKPLab/sentence-transformers)库进行模型搭建与实验。该框架提供了一种简便的方法来计算句子和段落的向量表示（也称为句子嵌入）。这些模型基于诸如BERT / RoBERTa / XLM-RoBERTa等模型，并且经过专门调整以优化向量表示，以使具有相似含义的句子在向量空间中接近。

实验数据

数据集为QA_corpus，训练数据10w条，验证集和测试集均为1w条。实例如下：

> [('为什么我无法看到额度', '为什么开通了却没有额度', 0),
> ('为啥换不了', '为两次还都提示失败呢', 0),
> ('借了钱，但还没有通过，可以取消吗？', '可否取消', 1),
> ('为什么我申请额度输入密码就一直是那个页面', '为什么要输入支付密码来验证', 0),
> ('今天借 明天还款可以？', '今天借明天还要手续费吗', 0),
> ('你好！今下午咱没有扣我款？', '你好 今天怎么没有扣款呢', 1),
> ('所借的钱是否可以提现？', '该笔借款可以提现吗！', 1),
> ('不是邀请的客人就不能借款吗', '一般什么样得人会受邀请', 0),
> ('人脸失别不了，开不了户', '我输入的资料都是正确的，为什么总说不符开户失败？', 0),
> ('一天利息好多钱', '1万利息一天是5元是吗', 1)]

1、实验代码

```
# 先安装sentence_transformers
from sentence_transformers import SentenceTransformer

# Define the model. Either from scratch of by loading a pre-trained model
model = SentenceTransformer('distiluse-base-multilingual-cased')
# distiluse-base-multilingual-cased 蒸馏得到的，官方预训练好的模型

# 加载数据集
def load_data(filename):
    D = []
    with open(filename, encoding='utf-8') as f:
        for l in f:
            try:
                text1, text2, label = l.strip().split(',')
                D.append((text1, text2, int(label)))
            except ValueError:
                _
    return D

train_data = load_data('text_matching/input/train.csv')
valid_data = load_data('text_matching/input/dev.csv')
test_data  = load_data('text_matching/input/test.csv')

from sentence_transformers import SentenceTransformer, SentencesDataset 
from sentence_transformers import InputExample, evaluation, losses
from torch.utils.data import DataLoader

# Define your train examples.
train_datas = []
for i in train_data:
    train_datas.append(InputExample(texts=[i[0], i[1]], label=float(i[2])))

# Define your evaluation examples
sentences1,sentences2,scores = [],[],[]
for i in valid_data:
    sentences1.append(i[0])
    sentences2.append(i[1])
    scores.append(float(i[2]))

evaluator = evaluation.EmbeddingSimilarityEvaluator(sentences1, sentences2, scores)


# Define your train dataset, the dataloader and the train loss
train_dataset = SentencesDataset(train_datas, model)
train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=16)
train_loss = losses.CosineSimilarityLoss(model)

# Define your train dataset, the dataloader and the train loss
train_dataset = SentencesDataset(train_datas, model)
train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=16)
train_loss = losses.CosineSimilarityLoss(model)

# Tune the model
model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=3, warmup_steps=100, 
          evaluator=evaluator, evaluation_steps=200, output_path='./two_albert_similarity_model')
```

2、向量相似度的测评：

```
# Define your evaluation examples
sentences1,sentences2,scores = [],[],[]
for i in test_data:
    sentences1.append(i[0])
    sentences2.append(i[1])
    scores.append(float(i[2]))

evaluator = evaluation.EmbeddingSimilarityEvaluator(sentences1, sentences2, scores)
model.evaluate(evaluator)
```

>>> 0.68723840499831
>>>
>>

3、模型准确度的测评：

```
'''
Evaluate a model based on the similarity of the embeddings by calculating the accuracy of 
identifying similar and dissimilar sentences. The metrics are the cosine similarity as well 
as euclidean and Manhattan distance The returned score is the accuracy with a specified metric.
'''
evaluator = evaluation.BinaryClassificationEvaluator(sentences1, sentences2, scores)
model.evaluate(evaluator)
```

>>> 0.8906211331111515
>>>
>>

4、测试模型获取向量

```
from sentence_transformers import SentenceTransformer, util

model = SentenceTransformer('./two_albert_similarity_model')

# Sentences are encoded by calling model.encode()
emb1 = model.encode('什么情况导致评估不过')
emb2 = model.encode("个人信用怎么评估出来的")
print(emb1)
print(emb2)

cos_sim = util.pytorch_cos_sim(emb1, emb2)
print("Cosine-Similarity:", cos_sim)
```

> emb1:[ 2.98918579e-02 -1.61327735e-01 -2.11362451e-01 -8.07176754e-02 8.48087892e-02 -1.54550061e-01 -1.11961491e-01 -7.36757461e-03 。。。 -1.64973773e-02 -6.62902296e-02 7.76088312e-02 -5.86621352e-02]
> emb2:[-0.00474379 0.01176221 -0.12961781 0.03897651 -0.08082788 0.02274037 -0.01527447 -0.03132218 0.04967966 -0.11125126 0.03260884 -0.08910057。。。 0.04023521 0.07583613 -0.01950659 -0.04246246 0.03055439 0.0451214] Cosine-Similarity: tensor([[-0.0515]])

5、模型向量召回

```
from tqdm import tqdm
import numpy as np
import faiss                   # make faiss available

ALL = []
for i in tqdm(test_data):
    ALL.append(i[0])
    ALL.append(i[1])
ALL = list(set(ALL))

DT = model.encode(ALL)
DT = np.array(DT, dtype=np.float32)

# https://waltyou.github.io/Faiss-Introduce/
index = faiss.IndexFlatL2(DT[0].shape[0])   # build the index
print(index.is_trained)
index.add(DT)                  # add vectors to the index
print(index.ntotal)
```

6、查询最相似的文本

```
k = 10                          # we want to see 10 nearest neighbors
aim = 220
D, I = index.search(DT[aim:aim+1], k) # sanity check
print(I)
print(D)
print([ALL[i]for i in I[0]])
```

[[ 220 4284 3830 2112 1748 639 5625 6062 1515 1396]]
[[0. 0.04789903 0.04982989 0.07678283 0.08252098 0.08306326
0.08532818 0.11053496 0.11116458 0.11140325]]
['4500元一天要多少息', '借一万一天利息多少钱', '万元日利息多少', '代五万元，一天的息是多少', '一万元的日息是多少？', '一万元每天多少钱利息', '1千元日息是多少', '一天利息好多钱', '10000块日利息是多少', '借1万一天多少钱利息啊']

7、先获取特征再查询最相似的文本

```
query = [model.encode('1万元的每天利息是多少')]
query = np.array(query, dtype=np.float32)
D, I = index.search(query, 10) # sanity check
print(I)
print(D)
print([ALL[i]for i in I[0]])
```

[[3170 1476 639 2112 1826 3193 1396 4332 5360 1748]]
[[0.03987426 0.05244656 0.05858241 0.05872107 0.07376505 0.08907703
0.0905426 0.09842405 0.10062639 0.10626719]]
['20000每天利息多少', '1万元日利息多少', '一万元每天多少钱利息', '代五万元，一天的息是多少', '1万元日息是多少啊！', '100000元一天的利息是5000吗', '借1万一天多少钱利息啊', '借一万八，那一天是多少利息', '28000的日息是多少', '一万元的日息是多少？']

## 与其他模型的对比

| 模型                                     | loss       | acc           |
| ---------------------------------------- | ---------- | ------------- |
| DSSM                                     | 0.7613157  | 0.6864        |
| ConvNet                                  | 0.6872447  | 0.6977        |
| ESIM                                     | 0.55444807 | 0.736         |
| ABCNN                                    | 0.5771452  | 0.7503        |
| BiMPM                                    | 0.4852     | 0.764         |
| DIIN                                     | 0.48298636 | 0.7694        |
| DRCN                                     | 0.6549849  | 0.7811        |
| SBERT(distiluse-base-multilingual-cased) | 0.6872384  | 0.8906 - 0.91 |

SBERT模型的准确率提升很多。

其他模型代码参考 [terrifyzhao/text_matching](https://link.zhihu.com/?target=https%3A//github.com/terrifyzhao/text_matching)

## 具体代码

```
from sentence_transformers import SentenceTransformer, SentencesDataset
from sentence_transformers import InputExample, evaluation, losses
from torch.utils.data import DataLoader
import pandas as pd
from tqdm import tqdm

model = SentenceTransformer('distiluse-base-multilingual-cased')

train_data = pd.read_csv(r"train.csv", sep="\t")
train_data.sample(frac=1)
val_data = pd.read_csv(r"val.csv", sep="\t")
val_data.sample(frac=1)
test_data = pd.read_csv(r"test.csv", sep="\t")

def get_input():
    train_datas = []
    _y = train_data["y"]
    _s1 = train_data["s1"]
    _s2 = train_data["s2"]
    for s1, s2, l in tqdm(zip(_s1, _s2, _y)):
        train_datas.append(InputExample(texts=[s1, s2], label=float(l)))
    return train_datas

train_datas = get_input()

def eval_examples():
    sentences1, sentences2, scores = [], [], []
    for s1, s2, l in tqdm(zip(val_data["s1"], val_data["s2"], val_data["y"])):
        sentences1.append(s1)
        sentences2.append(s2)
        scores.append(float(l))
    return sentences1, sentences2, scores

sentences1, sentences2, scores = eval_examples()


# Define your train dataset, the dataloader and the train loss
train_dataset = SentencesDataset(train_datas, model)
train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=64)
train_loss = losses.CosineSimilarityLoss(model)

evaluator = evaluation.BinaryClassificationEvaluator(sentences1, sentences2, scores)
# Tune the model
model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=50, warmup_steps=100,
          evaluator=evaluator, evaluation_steps=300, output_path='./two_albert_similarity_model')
model.evaluate(evaluator)

# Define your evaluation examples


def test_examples():
    sentences1, sentences2, scores = [], [], []
    for s1, s2, l in tqdm(zip(test_data["s1"], test_data["s2"], test_data["y"])):
        sentences1.append(s1)
        sentences2.append(s2)
        scores.append(float(l))
    return sentences1, sentences2, scores

sentences1, sentences2, scores = test_examples()

evaluator = evaluation.EmbeddingSimilarityEvaluator(sentences1, sentences2, scores)
print(model.evaluate(evaluator))

evaluator = evaluation.BinaryClassificationEvaluator(sentences1, sentences2, scores)
print(model.evaluate(evaluator))
```
